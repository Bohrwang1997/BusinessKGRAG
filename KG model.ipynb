{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3704516a-4f34-4937-8250-cda36e675f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after merging node and graph datasets:\nIndex(['~id', '~labels', 'fileName', 'errorMessage', 'fileSource',\n       'total_chunks', 'processingTime', 'createdAt', 'fileSize', 'nodeCount',\n       'model', 'processed_chunk', 'fileType', 'relationshipCount',\n       'is_cancelled', 'status', 'updatedAt', 'content_offset', 'page_number',\n       'length', 'id', 'text', 'position', 'embedding', 'description',\n       '~start_node_id', '~start_node_labels', '~start_node_property_fileName',\n       '~start_node_property_content_offset',\n       '~start_node_property_page_number', '~start_node_property_length',\n       '~start_node_property_id', '~start_node_property_text',\n       '~start_node_property_position', '~start_node_property_embedding',\n       '~relationship_type', '~end_node_id', '~end_node_labels',\n       '~end_node_property_fileName', '~end_node_property_errorMessage',\n       '~end_node_property_fileSource', '~end_node_property_total_chunks',\n       '~end_node_property_processingTime', '~end_node_property_createdAt',\n       '~end_node_property_fileSize', '~end_node_property_nodeCount',\n       '~end_node_property_model', '~end_node_property_processed_chunk',\n       '~end_node_property_fileType', '~end_node_property_relationshipCount',\n       '~end_node_property_is_cancelled', '~end_node_property_status',\n       '~end_node_property_updatedAt', '~start_node_property_errorMessage',\n       '~start_node_property_fileSource', '~start_node_property_total_chunks',\n       '~start_node_property_processingTime', '~start_node_property_createdAt',\n       '~start_node_property_fileSize', '~start_node_property_nodeCount',\n       '~start_node_property_model', '~start_node_property_processed_chunk',\n       '~start_node_property_fileType',\n       '~start_node_property_relationshipCount',\n       '~start_node_property_is_cancelled', '~start_node_property_status',\n       '~start_node_property_updatedAt', '~end_node_property_content_offset',\n       '~end_node_property_page_number', '~end_node_property_length',\n       '~end_node_property_id', '~end_node_property_text',\n       '~end_node_property_position', '~end_node_property_embedding',\n       '~end_node_property_description'],\n      dtype='object')\nColumns after merging relationships with node data:\nIndex(['~id_x', '~start_node_id', '~end_node_id', '~relationship_type',\n       '~id_y', '~labels_start', '~start_node_property_text_start',\n       '~start_node_property_embedding_start', '~id', '~labels_end',\n       '~start_node_property_text_end', '~start_node_property_embedding_end'],\n      dtype='object')\nFirst few rows of the cleaned and merged dataset:\n   ~start_node_id  ...  ~start_node_property_embedding_end\n0               1  ...                                 NaN\n1               1  ...                                 NaN\n2               1  ...                                 NaN\n3               1  ...                                 NaN\n4               1  ...                                 NaN\n\n[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "relationship_df = pd.read_csv('relationship-export.csv')\n",
    "node_df = pd.read_csv('node-export.csv')\n",
    "graph_df = pd.read_csv('graph-export.csv')\n",
    "\n",
    "# Merge node properties\n",
    "merged_node_df = pd.merge(node_df, graph_df, how='outer', left_on='~id', right_on='~start_node_id', suffixes=('_node', '_graph'))\n",
    "\n",
    "# Inspect the columns\n",
    "print(\"Columns after merging node and graph datasets:\")\n",
    "print(merged_node_df.columns)\n",
    "\n",
    "# Select only the existing columns\n",
    "columns_to_keep = ['~id', '~labels', \n",
    "                   'text_node', 'embedding_node', 'position_node', \n",
    "                   'length_node', 'content_offset_node', 'page_number_node', \n",
    "                   '~start_node_property_text', '~start_node_property_embedding']\n",
    "\n",
    "# Filter columns dynamically\n",
    "columns_to_keep_existing = [col for col in columns_to_keep if col in merged_node_df.columns]\n",
    "\n",
    "# Create the cleaned node dataframe\n",
    "cleaned_node_df = merged_node_df[columns_to_keep_existing]\n",
    "\n",
    "# Merge the cleaned node dataset with the relationship dataset using the '~start_node_id' and '~end_node_id'\n",
    "# attach the corresponding node properties to each relationship\n",
    "merged_relationship_df = pd.merge(relationship_df, cleaned_node_df, how='left', left_on='~start_node_id', right_on='~id')\n",
    "merged_relationship_df = pd.merge(merged_relationship_df, cleaned_node_df, how='left', left_on='~end_node_id', right_on='~id', suffixes=('_start', '_end'))\n",
    "\n",
    "# Inspect the columns after merging the relationships and node data\n",
    "print(\"Columns after merging relationships with node data:\")\n",
    "print(merged_relationship_df.columns)\n",
    "\n",
    "# select the columns for start and end node text and embeddings\n",
    "final_columns_to_keep = ['~start_node_id', '~end_node_id', '~relationship_type']\n",
    "\n",
    "# Filter columns that start with 'text_' or 'embedding_' for both start and end nodes\n",
    "final_columns_to_keep += [col for col in merged_relationship_df.columns if 'text_' in col or 'embedding_' in col]\n",
    "\n",
    "# Filter the final merged DataFrame\n",
    "final_merged_df = merged_relationship_df[final_columns_to_keep]\n",
    "\n",
    "# Clean up any remaining NaN or unnecessary rows\n",
    "final_cleaned_df = final_merged_df.dropna(subset=['~start_node_id', '~end_node_id'])\n",
    "\n",
    "# Save dataset\n",
    "final_cleaned_df.to_csv('final_merged_cleaned_dataset.csv', index=False)\n",
    "\n",
    "# Display\n",
    "print(\"First few rows of the cleaned and merged dataset:\")\n",
    "print(final_cleaned_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99849032-eb3d-4868-95b6-c0642efdf5ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the final dataset:\nIndex(['~start_node_id', '~end_node_id', '~relationship_type',\n       '~start_node_property_text_start',\n       '~start_node_property_embedding_start', '~start_node_property_text_end',\n       '~start_node_property_embedding_end'],\n      dtype='object')\nNode 1 text: Definitions accepted offer is acceptance is entering a written agreement the offer of placement is accepted within the nominated tim navigate accepted offers offer status a ie accepted includes cancelled studylink ltigtpending conditionsltigt created by australian government department of education esos framework admission is students admission into a program covering the point of admission and record of ongoing status of their adm does not include cancelled ltigtall conditions have been metltigt created by australian government dictionary advanced diploma is the purpose of the advanced diploma qualification type is to qualify individuals who apply specialised uk level 4 equivalent qualification knowledge of the underlying concepts and principles associated with their areas of study an ability to present alumni is a graduate or former student when a student completes their studies they become an alumni of the institution applicant is data relating to a person who has applied for at least one of the programs offered by the institution created b application is applicants instruction to be considered for a programme of study by submitting an application form\nNode 1 embedding: ['-0.028172967955470085','0.019017869606614113','0.026219958439469337','0.01593800261616707','0.020942941308021545','0.006162898149341345','0.050998784601688385','0.020823443308472633','-0.08031544089317322','0.06614826619625092','0.05613984167575836','-0.04334217682480812','-0.004547140095382929','-0.039915744215250015','-0.0013324860483407974','-0.044259924441576004','0.01040305383503437','-0.09709882736206055','0.015587955713272095','-0.01984531059861183','-0.011198262684047222','0.07321720570325851','-0.022854968905448914','-0.022676102817058563','0.007844693027436733','-0.03618866950273514','-0.023092595860362053','-0.02456909604370594','0.06899388134479523','0.05259081721305847','0.037106625735759735','0.07101025432348251','0.02679397352039814','0.04647974669933319','0.00358519796282053','-0.007929932326078415','-0.023452702909708023','-0.04702136665582657','0.007975861430168152','-0.04971747100353241','-0.05195419862866402','-0.041134510189294815','0.0020744637586176395','0.009722082875669003','0.03904532641172409','-0.050599079579114914','-0.07920722663402557','-0.0674157589673996','-0.06426838785409927','0.005212351214140654','0.014982000924646854','-0.022102363407611847','-0.0199865885078907','-0.02292484976351261','-0.09869107604026794','0.0532853789627552','-0.0653739869594574','-0.04439409449696541','-0.07949570566415787','-0.06617601215839386','-0.0069352202117443085','-0.015000686049461365','-0.0607023686170578','0.0314781628549099','-0.003605138510465622','-0.024189528077840805','-0.026839179918169975','-0.04342562332749367','0.09071459621191025','-0.0035777834709733725','-0.015132584609091282','-0.03249354287981987','-0.09482992440462112','0.029330536723136902','0.001668433891609311','-0.03281259164214134','0.05656290054321289','0.07385662198066711','0.03397778421640396','0.04935922101140022','0.043030157685279846','0.04670464247465134','-0.03642893582582474','-0.14043597877025604','0.04205866530537605','-0.06265056133270264','-0.020831039175391197','-0.02439059689640999','0.016388989984989166','0.04035288095474243','0.101197250187397','-0.08076498657464981','-0.029207447543740273','0.04300333932042122','0.09132813662290573','-0.07686923444271088','0.03650831803679466','-0.018356991931796074','0.013030221685767174','0.020683029666543007','-0.04411981999874115','-0.009299552999436855','-0.08420335501432419','0.050536029040813446','-0.10166516155004501','-0.050341904163360596','0.06724032759666443','-0.03411044180393219','0.05496739223599434','-0.06936823576688766','-0.05308985337615013','-0.0904928669333458','0.0054051862098276615','-0.006809370592236519','-0.03911710903048515','0.0581764280796051','0.005475470330566168','0.03100750595331192','0.07497861236333847','-0.03301405906677246','0.0008664622437208891','0.06999483704566956','0.03538576140999794','-0.09988804906606674','-0.03260250389575958','-0.14358772337436676','-0.050529394298791885','6.646097388071392e-33','-0.016791144385933876','0.011120593175292015','-0.09297961741685867','0.05951761081814766','0.00042329408461228013','0.035453539341688156','-0.02880859561264515','-0.016297442838549614','0.002513897605240345','0.028537122532725334','-0.003349883249029517','0.08530304580926895','-0.003950722515583038','0.07460566610097885','0.048818688839673996','0.05058910325169563','0.02111244760453701','0.1367010772228241','0.05821754038333893','0.06559091806411743','0.055047884583473206','-0.015854083001613617','0.03541262075304985','-0.02087322063744068','-0.03631691634654999','0.03504912182688713','-0.01690933294594288','0.019419344142079353','0.053641706705093384','0.005908680614084005','0.04234514757990837','-0.03871466964483261','-0.0751693919301033','-0.02572016604244709','-0.01136506162583828','0.023500530049204826','0.023273976519703865','-0.07599278539419174','0.07547155767679214','-0.08826064318418503','-0.03379078209400177','0.020414628088474274','0.006106698419898748','-0.01003993209451437','0.01650831662118435','-0.006883790250867605','-0.0027791541069746017','-0.0567048005759716','0.09353545308113098','0.11432981491088867','-0.016764555126428604','-0.054491665214300156','-0.06956464797258377','-0.10966577380895615','-0.016337038949131966','0.061160773038864136','-0.025687700137495995','0.0859609991312027','-0.050959210842847824','-0.017193002626299858','-0.0009402759023942053','-0.03661985322833061','-0.05237468704581261','-0.054956987500190735','-0.06028088182210922','-0.020640449598431587','0.03985082730650902','-0.13871070742607117','0.090415820479393','-0.03127922862768173','-0.09536673128604889','-0.08613455295562744','0.01924191601574421','0.060342058539390564','0.008108790032565594','-0.037503354251384735','-0.008128066547214985','0.054275669157505035','0.02714899741113186','0.0009112230036407709','-0.020070280879735947','0.01067533902823925','-0.043637365102767944','-0.06899987161159515','0.07563837617635727','0.04012152552604675','0.01695588231086731','-0.05143645033240318','0.08890791982412338','-0.02985156513750553','0.012027683667838573','-0.008452984504401684','-0.07812821865081787','0.09292788803577423','0.09297580271959305','-8.462041468839971e-33','0.10267551988363266','0.022341115400195122','-0.07349091023206711','-0.007120866794139147','0.027701547369360924','0.038732316344976425','0.06964611262083054','-0.01800842583179474','0.05589167773723602','-0.04050854593515396','0.031220924109220505','0.019761011004447937','0.02241053245961666','-0.012230566702783108','-0.05724877864122391','-0.03969210386276245','-0.006263649556785822','0.027603769674897194','-0.0006067108479328454','0.07213736325502396','0.02695477567613125','-0.012239832431077957','0.03718226030468941','-0.01647319458425045','0.015634309500455856','-0.02220294252038002','0.008841763250529766','-0.009022998623549938','-0.07861166447401047','-0.0824630856513977','0.02816709689795971','-0.038002077490091324','-0.12508289515972137','0.025387102738022804','-0.02926637977361679','-0.06894189119338989','0.08867664635181427','0.041050542145967484','-0.06044916436076164','0.11204636096954346','0.07942859828472137','-0.013412670232355595','0.005626242607831955','0.03419049456715584','-0.036780014634132385','0.009824912063777447','0.0541062094271183','-0.020986203104257584','0.07056805491447449','-0.02695431560277939','0.029234973713755608','0.05114981159567833','0.09266591817140579','0.00983204785734415','0.07923812419176102','-0.02796033024787903','0.0070516569539904594','-0.062493763864040375','-0.033598270267248154','0.03749074786901474','0.15367491543293','0.04969237744808197','0.02233101800084114','0.03052898682653904','0.032642610371112823','-0.05335911735892296','-0.03734061121940613','0.014162273146212101','-0.0862637460231781','0.0464802011847496','0.011372528038918972','-0.08328118920326233','0.008130593225359917','-0.08753093332052231','0.07870100438594818','-0.030737590044736862','0.0142895532771945','-0.02050360105931759','0.007598144467920065','-0.05933069810271263','-0.09253933280706406','0.06454267352819443','0.005420327186584473','0.05864011496305466','0.021948231384158134','-0.04810124263167381','0.001998470863327384','0.03037646785378456','0.07047386467456818','0.0076115974225103855','0.0247584767639637','-0.04600316286087036','-0.0019120711367577314','-0.030229318886995316','-0.008705844171345234','-5.027374072597013e-8','-0.04071991890668869','-0.029684126377105713','-0.08673864603042603','-0.0771213173866272','0.020334864035248756','0.04513343796133995','-0.07666192203760147','-0.0630277544260025','0.030869843438267708','-0.05220946669578552','-0.03902922198176384','-0.007453264202922583','-0.06933744996786118','-0.04649513587355614','0.04948916658759117','0.07018521428108215','-0.034859295934438705','0.0332380086183548','-0.007994349114596844','0.08027013391256332','-0.024038419127464294','-0.023684188723564148','0.012634147889912128','0.011562402360141277','0.01952526345849037','0.06692615896463394','0.024686332792043686','-0.010116239078342915','-0.017951954156160355','0.07203292101621628','-0.04266025498509407','-0.003663969924673438','0.12022440880537033','-0.03278873860836029','0.014004040509462357','0.001641793642193079','0.06212540343403816','-0.0434437058866024','-0.01906943880021572','0.027162889018654823','-0.033816538751125336','-0.04467558115720749','0.05736628547310829','0.027251768857240677','0.002545261988416314','0.08562608808279037','-0.054556164890527725','0.04880082607269287','-0.045899830758571625','-0.011643067933619022','-0.06745605915784836','-0.01696864329278469','-0.02725241892039776','-0.011685024946928024','-0.03393111377954483','-0.022047385573387146','-0.016292335465550423','-0.008921136148273945','-0.03926948830485344','-0.05754469707608223','0.08311387896537781','-0.028707755729556084','0.020254988223314285','0.00609670439735055']\nNodes connected to 1: [0, 2, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "final_cleaned_df = pd.read_csv('final_merged_cleaned_dataset.csv')\n",
    "\n",
    "# Inspect the columns to find the correct names for text and embedding columns\n",
    "print(\"Columns in the final dataset:\")\n",
    "print(final_cleaned_df.columns)\n",
    "\n",
    "# Create an empty directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes with their properties (text and embeddings)\n",
    "# Convert text and embedding values to strings, using placeholders for None values\n",
    "for index, row in final_cleaned_df.iterrows():\n",
    "    # Convert to string or use empty string if None\n",
    "    start_text = str(row.get('~start_node_property_text_start', ''))\n",
    "    start_embedding = str(row.get('~start_node_property_embedding_start', ''))\n",
    "    \n",
    "    end_text = str(row.get('~start_node_property_text_end', ''))\n",
    "    end_embedding = str(row.get('~start_node_property_embedding_end', ''))\n",
    "    \n",
    "    # Add start node with its properties\n",
    "    G.add_node(row['~start_node_id'], text=start_text, embedding=start_embedding)\n",
    "    \n",
    "    # Add end node with its properties\n",
    "    G.add_node(row['~end_node_id'], text=end_text, embedding=end_embedding)\n",
    "\n",
    "# Add edges (relationships) between nodes\n",
    "for index, row in final_cleaned_df.iterrows():\n",
    "    G.add_edge(row['~start_node_id'], row['~end_node_id'], relationship=row['~relationship_type'])\n",
    "\n",
    "# Save the graph structure (now it will work because all attributes are strings)\n",
    "nx.write_gml(G, 'kg_graph.gml')\n",
    "\n",
    "# Querying the graph\n",
    "# Query the graph to find relationships or node information\n",
    "example_node = final_cleaned_df.iloc[0]['~start_node_id']  \n",
    "print(f\"Node {example_node} text:\", G.nodes[example_node]['text'])\n",
    "print(f\"Node {example_node} embedding:\", G.nodes[example_node]['embedding'])\n",
    "\n",
    "connected_nodes = list(G.successors(example_node))\n",
    "print(f\"Nodes connected to {example_node}:\", connected_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0579487e-9199-443b-a932-a99569a79c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n  Obtaining dependency information for faiss-cpu from https://files.pythonhosted.org/packages/76/6c/256239bd142101cd2ce50d920622ab6d5a03742eabc462db49d7910c69c7/faiss_cpu-1.8.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading faiss_cpu-1.8.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: numpy<2.0,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from faiss-cpu) (1.23.5)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from faiss-cpu) (23.2)\nDownloading faiss_cpu-1.8.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/27.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/27.0 MB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\n\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/27.0 MB\u001B[0m \u001B[31m45.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/27.0 MB\u001B[0m \u001B[31m51.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.1/27.0 MB\u001B[0m \u001B[31m63.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.9/27.0 MB\u001B[0m \u001B[31m89.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.0/27.0 MB\u001B[0m \u001B[31m97.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m19.4/27.0 MB\u001B[0m \u001B[31m96.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m23.1/27.0 MB\u001B[0m \u001B[31m94.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m23.3/27.0 MB\u001B[0m \u001B[31m74.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m23.3/27.0 MB\u001B[0m \u001B[31m61.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m23.4/27.0 MB\u001B[0m \u001B[31m51.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m23.5/27.0 MB\u001B[0m \u001B[31m44.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m23.7/27.0 MB\u001B[0m \u001B[31m37.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m24.3/27.0 MB\u001B[0m \u001B[31m33.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m27.0/27.0 MB\u001B[0m \u001B[31m18.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.8.0.post1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef49b11-6ce7-4762-bbe0-ea8ab75d59d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44108dcff98547ad9023930551259c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8e9d4e9b474334bcba4b4b5df4561f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d6181b93c4400984abe91d5f8ac3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34397ace82d1415fb35de27d3c9ca3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c850bdfab304271a9e0c4772ef31fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875dc1fe255b4ae6a60757971d59075e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaab4fc8a4e40499e99d03164d95633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d474949103034ca68edea85147cc7665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d0f6bde07b4ed29b09dd86d6a0346c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb44b7e72cbc43359e81665f36ee14ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4281ef0bf03e44bb8846675839da36ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dd17b1eb1a4584acf2ed446a52c810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9cbe00eada45cc904cb317a4824a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6286219269a34c9b94d05b7037006962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082fdc610d4f4aa9bf3b3c7c7ded4e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63784661bd67417287f8aac7427c4275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1782d40c426b4d60acd8bffac778e172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n  warn_deprecated(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is accreditation?\nGenerated Reply:\n: What is accreditation? Relevant information: FAISS info: What is accreditation? Relevant information: FAISS info: What is accreditation? Relevant information: FAISS info: Academic Accreditation is Approval process by an accrediting authority of a learning program leading to an AQF qualification. OR the evaluation of a course of study against course requirements specified in the Higher Education Standards Framework.\nReference Answer:\nAcademic Accreditation is Approval process by an accrediting authority of a learning program leading to an AQF qualification using the quality assurance standards for the relevant education and training sector OR the evaluation of a course of study (either by TEQSA or by a self-accrediting provider such as a university) against course requirements specified in the Higher Education Standards Framework. created by AQF (Australian Qualifications Framework)\nBLEU Score: 0.4674996214456577\nMemory Usage: 1706.05 MB\nTime taken for this query: 11.04 seconds\n\nQuery: How is assessment done?\nGenerated Reply:\nAssessment Result is The mark given for an assessment item created by The University of Western Australia \"Examination is Summative assessment where a student is required to undertake a specified academic task within a specified period of time, normally between one and three hours. the location of the examination and access to external assistance is regulated. created by HESA\" \"Assessment is Assessment is the process of appraising, evaluating and making a judgment about\nReference Answer:\n\"Assessment is Assessment is the process of appraising, evaluating and making a judgment about someone's knowledge, skills and ability in order to determine if they have met with and achieved the designated learning outcomes of individual units of study.\nBLEU Score: 0.1903855063919507\nMemory Usage: 1706.06 MB\nTime taken for this query: 6.50 seconds\n\nQuery: What is a bachelor's degree?\nGenerated Reply:\n: \"Under Graduate Level is Academic program of study leading to a qualification, such as an associate degree, diploma or bachelor degree. Includes an honours degree. Undergraduate degrees are usually three or four years, and are also known as Bachelor’s degrees. created by Australian Government Dictionary\" Undergraduate qualifications offered by most higher education institutions - Bachelor degree: provides initial preparation for professional careers and postgraduate study and involve a minimum of three years of full-time study\nReference Answer:\n\"Under Graduate Level is Academic program of study leading to a qualification, such as an associate degree, diploma or bachelor degree. Includes an honours degree. Undergraduate degrees are usually three or four years, and are also known as Bachelor’s degrees. created by Australian Government Dictionary\"\nBLEU Score: 0.583765433019679\nMemory Usage: 1706.06 MB\nTime taken for this query: 6.94 seconds\n\nQuery: What is a campus?\nGenerated Reply:\n: \"Campus is Data relating to an institution's campus locations; the grounds and buildings where the institution conducts its learning and teaching, and research, activities OR a campus is the physical location from where a course or program of study is delivered. \"Managed Campus is A university campus managed by Navitas. Students are enrolled in the university and courses are self-accredited by the university. Navitas typically provides the premises, delivers\nReference Answer:\n\"Campus is Data relating to an institution's campus locations; the grounds and buildings where the institution conducts its learning and teaching, and research, activities OR a campus is the physical location from where a course or program of study is delivered. For on‑line/ distance education courses: the location where the course is administered. While some studies may require a student's attendance at locations such as a health centre, teaching hospital or agricultural farm, these are not campuses in the common understanding of a campus. In these circumstances the student load should be reported against the campus from where these studies are administered. created by Australian Government Department of Education and Training (HEIMS)\"\nBLEU Score: 0.33619492897407227\nMemory Usage: 1716.17 MB\nTime taken for this query: 6.81 seconds\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-9fc6b9f9d2a64216b04e7adb9d816df7\", \"tr-8cae4326ca6d4e389bfdf111b0386f68\", \"tr-2663d7722c864d6599b7bcd8ee13ef9e\", \"tr-02c9782bdba148358e2555aa6127bbc0\"]",
      "text/plain": [
       "[Trace(request_id=tr-9fc6b9f9d2a64216b04e7adb9d816df7), Trace(request_id=tr-8cae4326ca6d4e389bfdf111b0386f68), Trace(request_id=tr-2663d7722c864d6599b7bcd8ee13ef9e), Trace(request_id=tr-02c9782bdba148358e2555aa6127bbc0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss\n",
    "import time\n",
    "import psutil\n",
    "from functools import lru_cache\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Define Document class\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Load definitions from a text file\n",
    "def load_definitions(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Define the KG-augmented retriever class\n",
    "class KGAndFAISSRetriever:\n",
    "    def __init__(self, definitions, kg, num_retrieved_docs=5):\n",
    "        # FAISS document retrieval\n",
    "        all_documents = [Document(definition) for definition in definitions]\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        self.db = FAISS.from_documents(all_documents, embeddings)\n",
    "        self.retriever = self.db.as_retriever(search_kwargs={\"k\": num_retrieved_docs})\n",
    "        \n",
    "        # Knowledge Graph (KG)\n",
    "        self.kg = kg\n",
    "        self.num_retrieved_docs = num_retrieved_docs\n",
    "\n",
    "    def search(self, query):\n",
    "        # FAISS retrieval\n",
    "        faiss_docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # KG retrieval\n",
    "        kg_info = self.query_kg(query)\n",
    "        \n",
    "        return faiss_docs, kg_info\n",
    "\n",
    "    def query_kg(self, query):\n",
    "        # Query the KG for relevant nodes\n",
    "        relevant_nodes = []\n",
    "        for node in self.kg.nodes:\n",
    "            if query.lower() in self.kg.nodes[node]['text'].lower():\n",
    "                relevant_nodes.append(self.kg.nodes[node]['text'])\n",
    "        return \" \".join(relevant_nodes[:self.num_retrieved_docs])\n",
    "\n",
    "# Define the T5 Assistant class for generation\n",
    "class T5Assistant:\n",
    "    def __init__(self, model_name='t5-small'):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    def create_prompt(self, query, retrieved_info):\n",
    "        return (f\"Explain the concept or answer the question in a detailed manner using simple words and examples.\\n\"\n",
    "                f\"Instruction: {query}\\n\"\n",
    "                f\"Relevant information: {retrieved_info}\\n\"\n",
    "                f\"Output:\")\n",
    "\n",
    "    def generate_reply(self, query, retrieved_info):\n",
    "        prompt = self.create_prompt(query, retrieved_info)\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "        outputs = self.model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate BLEU score\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "\n",
    "# Print memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    print(f\"Memory Usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# LRU cache to speed up repeated queries\n",
    "@lru_cache(maxsize=10)\n",
    "def cached_generate_reply(assistant, query, retrieved_info):\n",
    "    return assistant.generate_reply(query, retrieved_info)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load definitions and KG\n",
    "    definitions = load_definitions('ctx_pd.txt')\n",
    "    \n",
    "    # Load the knowledge graph\n",
    "    kg = nx.read_gml('kg_graph.gml')\n",
    "    \n",
    "    # Initialize the retriever (with KG and FAISS)\n",
    "    retriever = KGAndFAISSRetriever(definitions, kg, num_retrieved_docs=5)\n",
    "    assistant = T5Assistant(model_name='t5-small')\n",
    "\n",
    "    # Sample queries\n",
    "    generated_queries = [\n",
    "        \"What is accreditation?\",\n",
    "        \"How is assessment done?\",\n",
    "        \"What is a bachelor's degree?\",\n",
    "        \"What is a campus?\"\n",
    "    ]\n",
    "\n",
    "    # Process each query\n",
    "    for query in generated_queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Retrieve from both FAISS and KG\n",
    "        faiss_docs, kg_info = retriever.search(query)\n",
    "        faiss_info = \" \".join([doc.page_content for doc in faiss_docs])\n",
    "        retrieved_info = f\"FAISS info: {faiss_info}\\nKG info: {kg_info}\"\n",
    "\n",
    "        # Find the reference answer\n",
    "        reference_answer = faiss_docs[0].page_content if faiss_docs else \"\"\n",
    "        \n",
    "        # Generate reply\n",
    "        generated_reply = cached_generate_reply(assistant, query, retrieved_info)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        if reference_answer:\n",
    "            bleu_score = calculate_bleu(reference_answer, generated_reply)\n",
    "        else:\n",
    "            bleu_score = \"N/A\"\n",
    "\n",
    "        # Output the results\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Generated Reply:\\n{generated_reply}\")\n",
    "        print(f\"Reference Answer:\\n{reference_answer}\")\n",
    "        print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "        print_memory_usage()\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken for this query: {end_time - start_time:.2f} seconds\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "KG model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
